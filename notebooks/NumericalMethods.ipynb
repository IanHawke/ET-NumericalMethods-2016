{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Numerical Relativity, we need to\n",
    "\n",
    "* evolve the spacetime (hyperbolic PDEs with \"smooth\" fields);\n",
    "* evolve the matter (hyperbolic PDEs with discontinuous fields);\n",
    "* solve initial data (elliptic PDEs);\n",
    "* extract gravitational waves (interpolation and integration);\n",
    "* find and analyse horizons (interpolation, BVPs).\n",
    "\n",
    "These can be built on some simple foundations. The crucial ones are\n",
    "\n",
    "1. the solution of linear systems $A {\\bf x} = {\\bf b}$;\n",
    "2. the solution of nonlinear root-finding problems ${\\bf f} ( {\\bf x} ) = {\\bf 0}$;\n",
    "3. the representation of a function or field $f(x)$ by discrete data $f_i$, by interpolation or other means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a black hole apparent horizon we need to find a surface where the expansion of outgoing null geodesics vanishes. In a simplified case (axisymmetric, time symmetric) we need to solve the BVP\n",
    "\n",
    "$$\n",
    "\\frac{d^2 h}{d \\theta^2} = 2 h + \\frac{3}{h} \\left( \\frac{d h}{d \\theta} \\right)^2 + f \\left( \\theta, h, \\frac{d h}{d \\theta} \\right), \\qquad \\frac{d h}{d \\theta} ( \\theta = 0 ) = 0 = \\frac{d h}{d \\theta} ( \\theta = \\pi ).\n",
    "$$\n",
    "\n",
    "Here $h$ is the horizon radius and $f$ a complicated function that depends on the singularities.\n",
    "\n",
    "To solve this we can\n",
    "\n",
    "1. represent $h$ as a discrete function of $\\theta$: $h_i = h(\\theta_i)$, and the representation interpolates the $h_i$;\n",
    "2. differentiate the interpolating representation of $h$ to approximate eg $\\frac{d h}{d \\theta}$, using *finite differencing*;\n",
    "3. use the finite differencing formula to solve the ODE with given initial data $\\frac{d h}{d \\theta}=0$ and a *guess* $h(\\theta = 0) = H_0$. This will give $h$ and its derivatives for all $\\theta$;\n",
    "4. solve the nonlinear root-finding problem given by $\\frac{d h}{d \\theta} \\left( \\theta = \\pi; H \\right) = 0$ (the *shooting method*);\n",
    "5. solve the ODE with initial data $h(\\theta = 0) = H_0$ and $\\frac{d h}{d \\theta}=0$.\n",
    "\n",
    "To extend this method away from axisymmetry we would need to extend all the systems, bringing in the matrices and the linear systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finite differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most formulations of the Einstein equations for the spacetime (with $c=1$) look roughly like *wave equations*\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 \\phi}{\\partial t^2} = \\nabla^2 \\phi.\n",
    "$$\n",
    "\n",
    "We will focus on the simple $1+1$d case\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 \\phi}{\\partial t^2} = \\frac{\\partial^2 \\phi}{\\partial x^2}.\n",
    "$$\n",
    "\n",
    "For numerical evolution we either write this as first order in time,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial t} \\begin{pmatrix} \\phi \\\\ \\phi_t \\end{pmatrix} = \\begin{pmatrix} \\phi_t \\\\ 0 \\end{pmatrix} + \\frac{\\partial^2}{\\partial x^2} \\begin{pmatrix} 0 \\\\ \\phi \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "or as first order in time *and* space\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial t} \\begin{pmatrix} \\phi \\\\ \\phi_t \\\\ \\phi_x \\end{pmatrix} = \\begin{pmatrix} \\phi_t \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{\\partial}{\\partial x} \\begin{pmatrix} 0 \\\\ \\phi_x \\\\ \\phi_t \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Write these as\n",
    "\n",
    "$$\n",
    "\\partial_t {\\bf u} = {\\bf s} + \\partial_x {\\bf f}({\\bf u}) + \\partial_{xx} {\\bf g}({\\bf u}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this numerically we need to replace the derivatives with something the computer can handle. Assume we know the values of ${\\bf u}$ at three equally spaced points $x_{i-1}, x_{i}, x_{i+1}$, grid spacing $\\Delta x$. Then if we interpolate ${\\bf u}$ using the *polynomial* ${\\bf U}$ we can approximate the derivatives of ${\\bf u}$ by the derivatives of ${\\bf U}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpolant\n",
    "\n",
    "$$\n",
    "{\\bf U}(x) = {\\bf u}(x_{i-1}) + \\frac{x - x_{i-1}}{\\Delta x} \\left( {\\bf u}(x_i) - {\\bf u}(x_{i-1}) \\right)\n",
    "$$\n",
    "\n",
    "is a linear polynomial (in $x$) that matches ${\\bf u}_{i-1}$ and ${\\bf u}_{i}$, so interpolates between these points. Its derivative gives the *backwards differencing* approximation\n",
    "\n",
    "$$\n",
    "\\left. \\frac{\\partial {\\bf u}}{\\partial x} \\right|_{x=x_i} \\simeq \\left. \\frac{\\partial {\\bf U}}{\\partial x} \\right|_{x=x_i} = \\frac{1}{\\Delta x} \\left( {\\bf u}_i - {\\bf u}_{i-1} \\right).\n",
    "$$\n",
    "\n",
    "Similarly, the interpolant\n",
    "\n",
    "$$\n",
    "{\\bf U}(x) = {\\bf u}(x_{i}) + \\frac{x - x_{i}}{\\Delta x} \\left( {\\bf u}(x_{i+1}) - {\\bf u}(x_{i}) \\right)\n",
    "$$\n",
    "\n",
    "is a linear polynomial (in $x$) that matches ${\\bf u}_{i}$ and ${\\bf u}_{i+1}$, whose derivative gives the *forwards differencing* approximation\n",
    "\n",
    "$$\n",
    "\\left. \\frac{\\partial {\\bf u}}{\\partial x} \\right|_{x=x_i} \\simeq \\left. \\frac{\\partial {\\bf U}}{\\partial x} \\right|_{x=x_i} = \\frac{1}{\\Delta x} \\left( {\\bf u}_{i+1} - {\\bf u}_{i} \\right).\n",
    "$$\n",
    "\n",
    "The second derivatives vanish, which isn't helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpolant\n",
    "\n",
    "$$\n",
    "{\\bf U}(x) = \\frac{(x - x_{i})(x - x_{i+1})}{2 (\\Delta x)^2} {\\bf u}_{i-1} - \\frac{(x - x_{i-1})(x - x_{i+1})}{(\\Delta x)^2} {\\bf u}_{i} + \\frac{(x - x_{i-1})(x - x_{i})}{2 (\\Delta x)^2} {\\bf u}_{i+1}\n",
    "$$\n",
    "\n",
    "is a quadratic that matches ${\\bf u}_{i-1},{\\bf u}_{i}$, and ${\\bf u}_{i+1}$, so interpolates between these points. Its derivative gives the (second order) *central differencing* approximations\n",
    "\n",
    "$$\n",
    "\\left. \\frac{\\partial {\\bf u}}{\\partial x} \\right|_{x=x_i} \\simeq \\left. \\frac{\\partial {\\bf U}}{\\partial x} \\right|_{x=x_i} = \\frac{1}{2 \\Delta x} \\left( {\\bf u}_{i+1} - {\\bf u}_{i+1} \\right)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\left. \\frac{\\partial^2 {\\bf u}}{\\partial x^2} \\right|_{x=x_i} \\simeq \\left. \\frac{\\partial^2 {\\bf U}}{\\partial x^2} \\right|_{x=x_i} = \\frac{1}{(\\Delta x)^2} \\left( {\\bf u}_{i-1} - 2 {\\bf u}_{i} + {\\bf u}_{i+1} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the accuracy of these approximations by applying them to a simple function. The exponentional $\\exp(x)$ has a derivative equalling itself, and when evaluated at zero should equal 1.\n",
    "\n",
    "First try with a large value of $\\Delta x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_differencing(f, x_i, dx):\n",
    "    \"\"\"\n",
    "    Backward differencing of f at x_i with grid spacing dx.\n",
    "    \"\"\"\n",
    "    f_i = f(x_i)\n",
    "    f_i_minus_1 = f(x_i - dx)\n",
    "    \n",
    "    return (f_i - f_i_minus_1) / dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_differencing(f, x_i, dx):\n",
    "    \"\"\"\n",
    "    Forward differencing of f at x_i with grid spacing dx.\n",
    "    \"\"\"\n",
    "    f_i = f(x_i)\n",
    "    f_i_plus_1 = f(x_i + dx)\n",
    "    \n",
    "    return (f_i_plus_1 - f_i) / dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def central_differencing(f, x_i, dx):\n",
    "    \"\"\"\n",
    "    Second order central differencing of f at x_i with grid spacing dx.\n",
    "    \"\"\"\n",
    "    f_i = f(x_i)\n",
    "    f_i_minus_1 = f(x_i - dx)\n",
    "    f_i_plus_1 = f(x_i + dx)\n",
    "    \n",
    "    first_derivative = (f_i_plus_1 - f_i_minus_1) / (2.0 * dx)\n",
    "    second_derivative = (f_i_minus_1 - 2.0 * f_i + f_i_plus_1) / (dx**2)\n",
    "    \n",
    "    return first_derivative, second_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bd = backward_differencing(exp, 0.0, dx=1.0)\n",
    "fd = forward_differencing(exp, 0.0, dx=1.0)\n",
    "cd1, cd2 = central_differencing(exp, 0.0, dx=1.0)\n",
    "\n",
    "print(\"Backward difference should be 1, is {}, error {}\".format(bd, abs(bd - 1.0)))\n",
    "print(\"Forward difference should be 1, is {}, error {}\".format(fd, abs(fd - 1.0)))\n",
    "print(\"Central difference (1st derivative) should be 1, is {}, error {}\".format(cd1, abs(cd1 - 1.0)))\n",
    "print(\"Central difference (2nd derivative) should be 1, is {}, error {}\".format(cd2, abs(cd2 - 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all errors are large, but central differencing is clearly the best. Let's try improving accuracy by reducing grid spacing by a factor of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bd = backward_differencing(exp, 0.0, dx=0.1)\n",
    "fd = forward_differencing(exp, 0.0, dx=0.1)\n",
    "cd1, cd2 = central_differencing(exp, 0.0, dx=0.1)\n",
    "\n",
    "print(\"Backward difference should be 1, is {}, error {}\".format(bd, abs(bd - 1.0)))\n",
    "print(\"Forward difference should be 1, is {}, error {}\".format(fd, abs(fd - 1.0)))\n",
    "print(\"Central difference (1st derivative) should be 1, is {}, error {}\".format(cd1, abs(cd1 - 1.0)))\n",
    "print(\"Central difference (2nd derivative) should be 1, is {}, error {}\".format(cd2, abs(cd2 - 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the backward and forward differencing errors are about the same, whilst the central differencing error is much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's most important is how rapidly the error goes to zero. We expect and hope that the error ${\\cal E}$ does go to zero, usually by being proportional to the grid spacing:\n",
    "\n",
    "$$\n",
    "{\\cal E} \\propto (\\Delta x)^p.\n",
    "$$\n",
    "\n",
    "We can experimentally check this by plotting the error against $\\Delta x$ on a suitable scale. As\n",
    "\n",
    "$$\n",
    "\\log \\left( {\\cal E} \\right) \\sim p \\log \\left( \\Delta x \\right) + \\text{constant}\n",
    "$$\n",
    "\n",
    "we can measure the *convergence rate* $p$ from the slope of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "%matplotlib notebook\n",
    "\n",
    "dxs = numpy.logspace(-5, 0, 10)\n",
    "bd_errors = numpy.zeros_like(dxs)\n",
    "fd_errors = numpy.zeros_like(dxs)\n",
    "cd1_errors = numpy.zeros_like(dxs)\n",
    "cd2_errors = numpy.zeros_like(dxs)\n",
    "\n",
    "for i, dx in enumerate(dxs):\n",
    "    bd_errors[i] = abs(backward_differencing(exp, 0.0, dx) - 1.0)\n",
    "    fd_errors[i] = abs(forward_differencing(exp, 0.0, dx) - 1.0)\n",
    "    cd1, cd2 = central_differencing(exp, 0.0, dx)\n",
    "    cd1_errors[i] = abs(cd1 - 1.0)\n",
    "    cd2_errors[i] = abs(cd2 - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.loglog(dxs, bd_errors, 'kx', label='Backwards')\n",
    "pyplot.loglog(dxs, fd_errors, 'b+', label='Forwards')\n",
    "pyplot.loglog(dxs, cd1_errors, 'go', label='Central (1st)')\n",
    "pyplot.loglog(dxs, cd2_errors, 'r^', label='Central (2nd)')\n",
    "pyplot.loglog(dxs, dxs*(bd_errors[0]/dxs[0]), 'k-', label=r\"$p=1$\")\n",
    "pyplot.loglog(dxs, dxs**2*(cd1_errors[0]/dxs[0]**2), 'k--', label=r\"$p=2$\")\n",
    "pyplot.xlabel(r\"$\\Delta x$\")\n",
    "pyplot.ylabel(\"Error\")\n",
    "pyplot.legend(loc=\"lower right\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forwards and backwards differencing are converging at first order ($p=1$). Central differencing is converging at second order ($p=2$) until floating point effects start causing problems at small $\\Delta x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wave equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple algorithm for the wave equation would replace the time derivatives with *forward* differencing and the spatial derivatives with *central* differencing. We'll use the notation ${\\bf u}(t=t^n, x=x_i) = {\\bf u}^n_i$. Thus\n",
    "\n",
    "$$\n",
    "\\partial_t {\\bf u} = {\\bf s} + \\partial_x {\\bf f}({\\bf u}) + \\partial_{xx} {\\bf g}({\\bf u}).\n",
    "$$\n",
    "\n",
    "becomes\n",
    "\n",
    "$$\n",
    " \\frac{1}{\\Delta t} \\left( {\\bf u}^{n+1}_i - {\\bf u}^{n}_i \\right) = {\\bf s} \\left( {\\bf u}^n_i \\right) + \\frac{1}{2 \\Delta x} \\left( {\\bf f} \\left( {\\bf u}^n_{i+1} \\right) - {\\bf f} \\left( {\\bf u}^n_{i-1} \\right) \\right) + \\frac{1}{(\\Delta x)^2} \\left( {\\bf g} \\left( {\\bf u}^n_{i-1} \\right) - 2 {\\bf g} \\left( {\\bf u}^n_{i} \\right) + {\\bf g} \\left( {\\bf u}^n_{i+1} \\right) \\right).\n",
    "$$\n",
    "\n",
    "Rearranging we get\n",
    "\n",
    "$$\n",
    " {\\bf u}^{n+1}_i = {\\bf u}^{n}_i + (\\Delta t) \\, {\\bf s} \\left( {\\bf u}^n_i \\right) + \\frac{\\Delta t}{2 \\Delta x} \\left( {\\bf f} \\left( {\\bf u}^n_{i+1} \\right) - {\\bf f} \\left( {\\bf u}^n_{i-1} \\right) \\right) + \\frac{\\Delta t}{(\\Delta x)^2} \\left( {\\bf g} \\left( {\\bf u}^n_{i-1} \\right) - 2 {\\bf g} \\left( {\\bf u}^n_{i} \\right) + {\\bf g} \\left( {\\bf u}^n_{i+1} \\right) \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would allow us to evolve a wave, provided we have *boundary conditions* (what happens at the edge of the computational domain), and *initial conditions* (what happens at $t=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple case we'll fix the domain to be $x \\in [-1, 1]$. We will focus on the first order in time, first order in space formulation. We will choose the boundaries to be *periodic*, so ${\\bf u}(t, x=-1) = {\\bf u}(t, x=1)$. We will also choose the initial data to be a time symmetric gaussian,\n",
    "\n",
    "$$\n",
    "\\phi(0, x) = \\exp \\left( -20 x^2 \\right), \\qquad \\partial_t \\phi (0, x) = 0,\n",
    "$$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$\n",
    "\\partial_x \\phi(0, x) = -40 x \\exp \\left( -20 x^2 \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define functions to set the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_data(x):\n",
    "    \"\"\"\n",
    "    Set the initial data. x are the coordinates. u (phi, phi_t, phi_x) are the variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    u = numpy.zeros((3, len(x)))\n",
    "    u[0, :] = numpy.exp(-20.0 * x**2)\n",
    "    u[2, :] = -40.0*x*numpy.exp(-20.0 * x**2)\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the grid, we will assume that points are staggered away from the boundaries. So the first point will be at $x = -1 - \\tfrac{1}{2} \\Delta x$, the second at $x = -1 + \\tfrac{1}{2} \\Delta x$, the last-but-one at $x = 1 - \\tfrac{1}{2} \\Delta x$ and the last at $x = 1 + \\tfrac{1}{2} \\Delta x$. The periodic boundary conditions mean that ${\\bf u}$ at the first point (which is *outside* the domain) must match ${\\bf u}$ at the last-but-one point (which is *inside* the domain). Similarly, the periodic boundary conditions mean that ${\\bf u}$ at the last point (which is *outside* the domain) must match ${\\bf u}$ at the second point (which is *inside* the domain).\n",
    "\n",
    "We define a function for the grid and the boundary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid(Npoints):\n",
    "    \"\"\"\n",
    "    Npoints is the number of interior points\n",
    "    \"\"\"\n",
    "    \n",
    "    dx = 2.0 / Npoints\n",
    "    return dx, numpy.linspace(-1.0-dx/2.0, 1.0+dx/2.0, Npoints+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_bcs(u):\n",
    "    \"\"\"\n",
    "    Apply periodic boundary conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    u[:, 0] = u[:, -2]\n",
    "    u[:, -1] = u[:, 1]\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a function that computes the *update* ${\\bf u}^{n+1}_i - {\\bf u}^n_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(u, dt, dx):\n",
    "    \"\"\"\n",
    "    Compute the update term for the wave equation.\n",
    "    \n",
    "    Remember, u contains (phi, phi_t)\n",
    "    \"\"\"\n",
    "    \n",
    "    update = numpy.zeros_like(u)\n",
    "    \n",
    "    update[0, :] = dt * u[1, :]\n",
    "    update[1, 1:-1] = dt / (2.0*dx) * (u[2,2:] - u[2,0:-2])\n",
    "    update[2, 1:-1] = dt / (2.0*dx) * (u[1,2:] - u[1,0:-2])\n",
    "    \n",
    "    return update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could put this together if we choose the timestep. For now, we'll choose $\\Delta t = \\tfrac{1}{10} \\Delta x$. So let's evolve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Npoints = 100\n",
    "t_end = 1.0\n",
    "cfl = 0.2\n",
    "dx, x = grid(Npoints)\n",
    "dt = cfl * dx\n",
    "Nsteps = int(t_end/dt)\n",
    "u = initial_data(x)\n",
    "u_old = numpy.zeros_like(u)\n",
    "u_old[:,:] = u[:,:]\n",
    "\n",
    "t = 0.0\n",
    "for n in range(Nsteps):\n",
    "    t += dt\n",
    "    u_old[:,:] = u[:,:]\n",
    "    u = u_old + update(u_old, dt, dx)\n",
    "    u = apply_bcs(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, u[0,:], 'bx-')\n",
    "pyplot.xlim(-1.0,1.0)\n",
    "pyplot.xlabel(r\"$x$\")\n",
    "pyplot.ylabel(r\"$\\phi$\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything appears to be working nicely - the pulse has moved left and right at speed 1. If we keep evolving to $t=2$ then, thanks to the periodic boundaries, we should get the initial data back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Npoints = 100\n",
    "t_end = 2.0\n",
    "cfl = 0.2\n",
    "dx, x = grid(Npoints)\n",
    "dt = cfl * dx\n",
    "Nsteps = int(t_end/dt)\n",
    "u = initial_data(x)\n",
    "u_old = numpy.zeros_like(u)\n",
    "u_old[:,:] = u[:,:]\n",
    "\n",
    "t = 0.0\n",
    "for n in range(Nsteps):\n",
    "    t += dt\n",
    "    u_old[:,:] = u[:,:]\n",
    "    u = u_old + update(u_old, dt, dx)\n",
    "    u = apply_bcs(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, u[0,:], 'bx-')\n",
    "pyplot.xlim(-1.0,1.0)\n",
    "pyplot.xlabel(r\"$x$\")\n",
    "pyplot.ylabel(r\"$\\phi$\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't quick as the timestep is so small. Let's try increasing the timestep so that $\\Delta t = \\Delta x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Npoints = 100\n",
    "t_end = 2.0\n",
    "cfl = 1.0\n",
    "dx, x = grid(Npoints)\n",
    "dt = cfl * dx\n",
    "Nsteps = int(t_end/dt)\n",
    "u = initial_data(x)\n",
    "u_old = numpy.zeros_like(u)\n",
    "u_old[:,:] = u[:,:]\n",
    "\n",
    "t = 0.0\n",
    "for n in range(Nsteps):\n",
    "    t += dt\n",
    "    u_old[:,:] = u[:,:]\n",
    "    u = u_old + update(u_old, dt, dx)\n",
    "    u = apply_bcs(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, u[0,:], 'bx-')\n",
    "pyplot.xlim(-1.0,1.0)\n",
    "pyplot.xlabel(r\"$x$\")\n",
    "pyplot.ylabel(r\"$\\phi$\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work as the algorithm is unstable when the timestep is too large. The limit, often referred to as the Courant-Friedrichs-Levy (CFL) limit, depends on the maximum propagation speed (here $c=1$) and the numerical method. Usually we expect $\\Delta t < \\Delta x / c$ at best - a CFL limit of $1$ - but often a CFL limit of $1/2$ or $1/4$ is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get a more accurate result by increasing the resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Npoints = 400\n",
    "t_end = 2.0\n",
    "cfl = 0.2\n",
    "dx, x = grid(Npoints)\n",
    "dt = cfl * dx\n",
    "Nsteps = int(t_end/dt)\n",
    "u = initial_data(x)\n",
    "u_old = numpy.zeros_like(u)\n",
    "u_old[:,:] = u[:,:]\n",
    "\n",
    "t = 0.0\n",
    "for n in range(Nsteps):\n",
    "    t += dt\n",
    "    u_old[:,:] = u[:,:]\n",
    "    u = u_old + update(u_old, dt, dx)\n",
    "    u = apply_bcs(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, u[0,:], 'bx-')\n",
    "pyplot.xlim(-1.0,1.0)\n",
    "pyplot.xlabel(r\"$x$\")\n",
    "pyplot.ylabel(r\"$\\phi$\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks horrible. The problem is that the numerical method isn't stable. The simplest way around this is to change the way we approximate the time derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *Method of Lines* we discretize first in space. This converts our *partial* differential equation, with derivatives in time and space, to an *ordinary* different equation, with derivatives only in time. We can then use any numerical approximation on the time part.\n",
    "\n",
    "Here we will use second order Runge-Kutta. If we have the ODE\n",
    "\n",
    "$$\n",
    "\\frac{d {\\bf u}}{d t} = {\\bf F}({\\bf u})\n",
    "$$\n",
    "\n",
    "then the RK2 method is\n",
    "\n",
    "\\begin{align}\n",
    "  {\\bf u}^{(1)} &= {\\bf u}^{n} + \\Delta t \\, {\\bf F}( {\\bf u}^{n} ), \\\\\n",
    "  {\\bf u}^{n+1} &= \\frac{1}{2} \\left( {\\bf u}^{n} + {\\bf u}^{(1)} + \\Delta t \\, {\\bf F}( {\\bf u}^{(1)} ) \\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing this the `update` function gives us $\\Delta t {\\bf F}$. However, we need to remember to impose the boundary conditions are each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Npoints = 400\n",
    "t_end = 2.0\n",
    "cfl = 0.2\n",
    "dx, x = grid(Npoints)\n",
    "dt = cfl * dx\n",
    "Nsteps = int(t_end/dt)\n",
    "u = initial_data(x)\n",
    "u_old = numpy.zeros_like(u)\n",
    "u1 = numpy.zeros_like(u)\n",
    "u_old[:,:] = u[:,:]\n",
    "\n",
    "t = 0.0\n",
    "for n in range(Nsteps):\n",
    "    t += dt\n",
    "    u_old[:,:] = u[:,:]\n",
    "    u1 = u_old + update(u_old, dt, dx)\n",
    "    u1 = apply_bcs(u1)\n",
    "    u = 0.5 * (u_old + u1 + update(u1, dt, dx))\n",
    "    u = apply_bcs(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, u[0,:], 'bx-')\n",
    "pyplot.xlim(-1.0,1.0)\n",
    "pyplot.xlabel(r\"$x$\")\n",
    "pyplot.ylabel(r\"$\\phi$\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally check its behaviour at much higher resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Npoints = 1600\n",
    "t_end = 2.0\n",
    "cfl = 0.2\n",
    "dx, x = grid(Npoints)\n",
    "dt = cfl * dx\n",
    "Nsteps = int(t_end/dt)\n",
    "u = initial_data(x)\n",
    "u_old = numpy.zeros_like(u)\n",
    "u1 = numpy.zeros_like(u)\n",
    "u_old[:,:] = u[:,:]\n",
    "\n",
    "t = 0.0\n",
    "for n in range(Nsteps):\n",
    "    t += dt\n",
    "    u_old[:,:] = u[:,:]\n",
    "    u1 = u_old + update(u_old, dt, dx)\n",
    "    u1 = apply_bcs(u1)\n",
    "    u = 0.5 * (u_old + u1 + update(u1, dt, dx))\n",
    "    u = apply_bcs(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, u[0,:], 'bx-')\n",
    "pyplot.xlim(-1.0,1.0)\n",
    "pyplot.xlabel(r\"$x$\")\n",
    "pyplot.ylabel(r\"$\\phi$\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that, at $t=2$, the solution should perfectly match the initial data. We should check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Npoints = 100\n",
    "t_end = 2.0\n",
    "cfl = 0.2\n",
    "dx, x = grid(Npoints)\n",
    "dt = cfl * dx\n",
    "Nsteps = int(t_end/dt)\n",
    "u_initial = initial_data(x)\n",
    "u = initial_data(x)\n",
    "u_old = numpy.zeros_like(u)\n",
    "u1 = numpy.zeros_like(u)\n",
    "u_old[:,:] = u[:,:]\n",
    "\n",
    "t = 0.0\n",
    "for n in range(Nsteps):\n",
    "    t += dt\n",
    "    u_old[:,:] = u[:,:]\n",
    "    u1 = u_old + update(u_old, dt, dx)\n",
    "    u1 = apply_bcs(u1)\n",
    "    u = 0.5 * (u_old + u1 + update(u1, dt, dx))\n",
    "    u = apply_bcs(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, u[0,:], 'bx', x, u_initial[0,:], 'k-')\n",
    "pyplot.xlim(-1.0,1.0)\n",
    "pyplot.xlabel(r\"$x$\")\n",
    "pyplot.ylabel(r\"$\\phi$\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they are similar but not identical. We will reduce the error to a single number by taking the norm of the error vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_norms(u, u_initial):\n",
    "    \"\"\"\n",
    "    Error norms (1, 2, infinity)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(u)\n",
    "    error_1 = numpy.sum(numpy.abs(u-u_initial))/N\n",
    "    error_2 = numpy.sqrt(numpy.sum((u-u_initial)**2)/N)\n",
    "    error_inf = numpy.max(numpy.abs(u-u_initial))\n",
    "    \n",
    "    return error_1, error_2, error_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_1, error_2, error_inf = error_norms(u_initial[0, 1:-1], u[0, 1:-1])\n",
    "print(\"Error norms are {} (1), {} (2), {} (infinity)\".format(error_1, error_2, error_inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check how the 2-norm error varies as we change the number of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Npoints_all = 100*2**numpy.arange(0,5)\n",
    "dxs = numpy.zeros((len(Npoints_all,)))\n",
    "wave_errors = numpy.zeros((len(Npoints_all,)))\n",
    "\n",
    "xs = []\n",
    "final_errors = []\n",
    "\n",
    "t_end = 2.0\n",
    "cfl = 0.2\n",
    "\n",
    "for i, Npoints in enumerate(Npoints_all):\n",
    "    dx, x = grid(Npoints)\n",
    "    dxs[i] = dx\n",
    "    dt = cfl * dx\n",
    "    Nsteps = int(t_end/dt)\n",
    "    u_initial = initial_data(x)\n",
    "    u = initial_data(x)\n",
    "    u_old = numpy.zeros_like(u)\n",
    "    u1 = numpy.zeros_like(u)\n",
    "    u_old[:,:] = u[:,:]\n",
    "\n",
    "    t = 0.0\n",
    "    for n in range(Nsteps):\n",
    "        t += dt\n",
    "        u_old[:,:] = u[:,:]\n",
    "        u1 = u_old + update(u_old, dt, dx)\n",
    "        u1 = apply_bcs(u1)\n",
    "        u = 0.5 * (u_old + u1 + update(u1, dt, dx))\n",
    "        u = apply_bcs(u)\n",
    "        \n",
    "    xs.append(x)\n",
    "    final_errors.append(u[0,:] - u_initial[0,:])\n",
    "    error_1, error_2, error_inf = error_norms(u_initial[0, 1:-1], u[0, 1:-1])\n",
    "    wave_errors[i] = error_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.loglog(dxs, wave_errors, 'bx', label=\"Errors\")\n",
    "pyplot.loglog(dxs, (dxs/dxs[-1])**4*wave_errors[-1], 'k-', label=r\"$p=4$\")\n",
    "pyplot.xlabel(r\"$\\Delta x$\")\n",
    "pyplot.ylabel(\"Error\")\n",
    "pyplot.legend(loc=\"lower right\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This miraculously converges at fourth order! This is a consequence of the perfect symmetry of the initial data, domain, and numerical method. If we just change the initial data by adding a small asymmetric piece (that still respects the periodicity), we get something rather different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initial_data_asymmetric(x):\n",
    "    \"\"\"\n",
    "    Set the initial data. x are the coordinates. u (phi, phi_t, phi_x) are the variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    u = numpy.zeros((3, len(x)))\n",
    "    u[0, :] = numpy.exp(-20.0 * x**2) + 0.2*(x+1.0)**3*(x-1.0)**2\n",
    "    u[2, :] = -40.0*x*numpy.exp(-20.0 * x**2) + 0.2*(3.0*(x-1.0)**2*(x+1.0)**2 + 2.0*(x-1.0)*(x+1.0)**3)\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Npoints_all = 100*2**numpy.arange(0,7)\n",
    "dxs = numpy.zeros((len(Npoints_all,)))\n",
    "wave_errors = numpy.zeros((len(Npoints_all,)))\n",
    "\n",
    "t_end = 2.0\n",
    "cfl = 0.2\n",
    "\n",
    "for i, Npoints in enumerate(Npoints_all):\n",
    "    dx, x = grid(Npoints)\n",
    "    dxs[i] = dx\n",
    "    dt = cfl * dx\n",
    "    Nsteps = int(t_end/dt)\n",
    "    u_initial = initial_data_asymmetric(x)\n",
    "    u = initial_data_asymmetric(x)\n",
    "    u_old = numpy.zeros_like(u)\n",
    "    u1 = numpy.zeros_like(u)\n",
    "    u_old[:,:] = u[:,:]\n",
    "\n",
    "    t = 0.0\n",
    "    for n in range(Nsteps):\n",
    "        t += dt\n",
    "        u_old[:,:] = u[:,:]\n",
    "        u1 = u_old + update(u_old, dt, dx)\n",
    "        u1 = apply_bcs(u1)\n",
    "        u = 0.5 * (u_old + u1 + update(u1, dt, dx))\n",
    "        u = apply_bcs(u)\n",
    "        \n",
    "    xs.append(x)\n",
    "    final_errors.append(u[0,:] - u_initial[0,:])\n",
    "    error_1, error_2, error_inf = error_norms(u_initial[0, 1:-1], u[0, 1:-1])\n",
    "    wave_errors[i] = error_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.plot(x, u[0,:], 'bx', x, u_initial[0,:], 'k-')\n",
    "pyplot.xlim(-1.0,1.0)\n",
    "pyplot.xlabel(r\"$x$\")\n",
    "pyplot.ylabel(r\"$\\phi$\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure()\n",
    "pyplot.loglog(dxs, wave_errors, 'bx', label=\"Errors\")\n",
    "pyplot.loglog(dxs, (dxs/dxs[-1])**2*wave_errors[-1], 'k-', label=r\"$p=2$\")\n",
    "pyplot.xlabel(r\"$\\Delta x$\")\n",
    "pyplot.ylabel(\"Error\")\n",
    "pyplot.legend(loc=\"lower right\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence is not perfectly second order, but we can see that it's much closer to the expected rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
